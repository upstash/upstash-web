---
title: "Integrating Vector Search in E-commerce Platforms with Upstash Vector"
slug: integrating-vector-search-in-e-commerce-platforms-with-upstash-vector
authors:
  - rishi
tags: [vector, rag, upstash, astro]
---

In this step-by-step guide, I talk about how I built a Tweet Scheduler using Upstash QStash, Upstash Redis, Next.js Server Actions and Vercel. Scheduling Twitter posts helps you maintain a consistent presence, engage with your audience at optimal times, and efficiently manage your content strategy.

## Prerequisites

You will need the following:

- [Node.js 18](https://nodejs.org/en/blog/announcements/v18-release-announce) or later
- An [Upstash](https://upstash.com) account

## Tech Stack

Following technologies are used in this guide:

| Technology | Description |
| --- | --- |
| [Upstash](https://upstash.com) | Serverless database platform. You're going to use Upstash Vector for storing vector embeddings and metadata. |
| [FastAPI](https://fastapi.tiangolo.com) | A high performance framework to build APIs with Python 3.8+. |
| [Astro](https://astro.build) | Framework for building fast, modern websites with serverless backend support. |
| [TailwindCSS](https://tailwindcss.com) | CSS framework for building custom designs. |

## Table Of Contents

- [Create an Upstash Vector Index](#create-an-upstash-vector-index)
- [Create a new FastAPI application](#create-a-new-fastapi-application)
- [Generate Text Embeddings using CLIPTextModel](#generate-text-embeddings-using-cliptextmodel)
- [Generate Image Embeddings using CLIPModel](#generate-image-embeddings-using-clipmodel)
- [Dynamically update Upstash Vector Index with new products](#dynamically-update-upstash-vector-index-with-new-products)
- [Dynamically search from Upstash Vector Index for similar products](#dynamically-search-from-upstash-vector-index-for-similar-products)
- [Create a new Astro application](#create-a-new-astro-application)

## Create an Upstash Vector Index

Once you have created an Upstash account and are logged in, go to the Vector tab and click on **Create Index** to start creating a vector index.

![Create an Upstash Vector](/blog/integrating-vector-search-in-e-commerce-platforms-with-upstash-vector/vector-tab.png)

Enter the index name of your choice (say, `products`) and set the vector dimensions to be of 512.

![Create An Upstash Vector Index](/blog/integrating-vector-search-in-e-commerce-platforms-with-upstash-vector/vector-create.png)

Now, scroll down till the **Connect** section, and click the **.env** button. Copy the contents, and save it somewhere safe to be used further in your application.

![Vector Index Environment Variables](/blog/integrating-vector-search-in-e-commerce-platforms-with-upstash-vector/index.png)

## Create a new FastAPI application

Letâ€™s get started by creating a new FastAPI project. Open your terminal and run the following commands:

```bash
# Create and move to the new directory
mkdir ecommerce-vector-search-server
cd ecommerce-vector-search-server

# Create a main.py file
touch main.py
```

The commands above create a directory named `ecommerce-vector-search-server` with `main.py` file in it.

Once you have cloned the repo, create an .env file. You are going to add the secret keys obtained in the sections above.

The .env file should contain the following keys:

```bash
# .env
 
# Upstash Vector Keys
UPSTASH_VECTOR_REST_URL="https://...-us1-vector.upstash.io"
UPSTASH_VECTOR_REST_TOKEN="...="
```

Next, in your first terminal window, run the command below to install the necessary libraries for building the application:

```bash
pip install fastapi "uvicorn[standard]"
pip install upstash_vector
pip install python_dotenv torch pillow transformers numpy
```

The above command installs the following packages:

- `fastapi`: A modern, fast (high-performance), web framework for building APIs with Python 3.6+ based on standard Python type hints.
- `uvicorn[standard]`: A lightning-fast ASGI server. The `[standard]` part indicates that you're installing Uvicorn with the standard set of dependencies.
- `upstash_vector`: A serverless Vector SDK (python) from Upstash.
- `python_dotenv`: A package to manage environment variables in a .env file.
- `torch`: A popular open-source machine learning library for Python, PyTorch.
- `pillow`: The Python Imaging Library (PIL) for opening many different image file formats.
- `numpy`: A package for scientific computing with Python.
- `transformers`: A library (by Hugging Face) for Natural Language Processing (NLP) tasks, providing implementations of state-of-the-art pre-trained models such as BERT, GPT, and others.

Once they are installed, let's move on to creating a function to generate vector embeddings from text input(s).

## Generate Text Embeddings using CLIPTextModel

Let's define a function to transform text into embeddings using the CLIPText model. Append the following code to `main.py` file:

```python
import torch
from torchvision import transforms
from transformers import CLIPProcessor, CLIPTextModel

# Initialize the CLIP processor
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# Initialize the CLIPTextModel
text_model = CLIPTextModel.from_pretrained("openai/clip-vit-base-patch32")

# Function to generate vector embeddings from text
def getTextEmbeddings(text):
    pooled_output = []
    inputs = processor(text, return_tensors="pt", padding=True)
    with torch.no_grad():
        text_features = text_model(**inputs)
        pooled_output = text_features.pooler_output
    return pooled_output[0]
```

The code above does the following:

- Initializes CLIPProcessor, which preprocesses text inputs for the CLIP model. It is using the **openai/clip-vit-base-patch32** variant.
- Initializes CLIPTextModel, which represents the text-processing component of the CLIP model, using the same variant as the processor.
- Creates a function named `getTextEmbeddings` that takes a text input, preprocesses it using the initialized processor, feeds it into the CLIPTextModel, and returns the pooled output, which represents the vector embedding of the text.

Let's move on to generating vector embeddings of images with CLIP model.

## Generate Image Embeddings using CLIPModel

Let's define a function (along with some helper functions) to fetch, and transform images into embeddings using the CLIP model. Append the following code to `main.py` file:

```python
import torch
import requests
from PIL import Image
from torchvision import transforms
from transformers import CLIPProcessor, CLIPModel, CLIPTextModel

# Initialize the CLIPModel
image_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")

# Image preprocesser
preprocess = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Function to fetch remote image
def getImage(url):
    res = requests.get(url, stream=True)
    return Image.open(res.raw)
 
# Function to generate vector embeddings from images
def getImageEmbeddings(image):
    image = preprocess(image)
    image = image.unsqueeze(0)
    with torch.no_grad():
        features = image_model.get_image_features(pixel_values=image)
    embedding = features.squeeze().cpu().numpy()
    return embedding.astype(np.float32)
```

The code above does the following:

- Initializes CLIPModel, which represents the image-processing component of the CLIP model, using the **openai/clip-vit-base-patch32** variant.
- Defines a series of transformations using `torchvision.transforms` to preprocess the input image, including resizing, conversion to tensor, and normalization.
- Creates a `getImage` function that takes a URL as input, fetches the corresponding image from the internet, and returns it as a PIL image object.
- Creates a `getImageEmbeddings` function that preprocesses the input image using the defined transformations, feeds it into the CLIPModel to obtain image features, and returns the embedding as a numpy array.

With that done, let's use both the vector embeddings generator functions in FastAPI endpoints.

## Dynamically update Upstash Vector Index with new products

With increasing inventory, e-commerce are ever growing. Hence, to visualize things like in real-time, you are going to create an API that would dynamically create embeddings of the products from it's attributes such as image, title and description and store them in the Upstash Vector index. This helps keeps the data up-to-date, resulting in accurate responses to user searches. Append the following code to `main.py` file:

```python
@app.post("/api/add")
async def add(request: Request):
    # Load JSON data from the request body
    request_body = await request.json()
    products = request_body['products']
    # Loop over each product
    for product in products:
        embedding = []
        # If the product has an image field
        if product["image"]:
            # Generate vector embeddings of the image of the product
            image = getImage(product["image"])
            embedding = getImageEmbeddings(image)
            # Insert the vector embeddings along with metadata into Upstash Vector
            index.upsert(vectors = [(random.random(), embedding.tolist(), product)])
        if product["title"]:
            # Generate vector embeddings of the title of the product
            text = product["title"]
            embedding = getTextEmbeddings(text)
            # Insert the vector embeddings along with metadata into Upstash Vector
            index.upsert(vectors = [(random.random(), embedding.tolist(), product)])
        if product["description"]:
            # Generate vector embeddings of the description of the product
            description = product["description"]
            embedding = getTextEmbeddings(description)
            # Insert the vector embeddings along with metadata into Upstash Vector
            index.upsert(vectors = [(random.random(), embedding.tolist(), product)])
```

The code above does the following:

- The endpoint parses the request body to extract product data.
- For each product, it checks if it has an image, title, or description field.
- If present, it generates vector embeddings using `getImageEmbeddings` or `getTextEmbeddings` functions.
- It then inserts these embeddings into Upstash Vector index along with metadata using the upsert method, ensuring that the index is dynamically updated with new product information.

## Dynamically search from Upstash Vector Index for similar products

For the UI to render rich product information responses to user queries, you are going to return the relevant product information by creating an endpoint it can talk to. Such an endpoint would be responsible to search for the relevant products based on the user prompt, including link to images. Append the following code to `main.py` file:

```python
@app.post("/api/chat")
async def chat(request: Request):
    # Load JSON data from the request body
    request_body = await request.json()
    messages = request_body['messages']
    # Get the latest user message
    user_input = messages[-1]['content']
    query_embedding = []
    # If image URL was passed in the latest user query and get it's vector embeddings
    if "https://" in user_input:
        query_embedding = getImageEmbeddings(getImage(user_input))
    # Else assume text and get it's vector embeddings
    else:
        query_embedding = getTextEmbeddings(user_input)
    # Return the top 3 relevant vectors along with their metadata
    output = index.query(vector=query_embedding.tolist(),  top_k=3, include_metadata=True)
    return [item for item in output if item.score > 0.8]
```

The code above does the following:

- The endpoint parses the request body to extract user messages.
- It retrieves the latest user input from the messages.
- If the input contains an image URL, it generates embeddings using `getImageEmbeddings`; otherwise, it uses `getTextEmbeddings` for text input.
- The generated embeddings are used to query the Upstash Vector index with the query method, specifying to retrieve the top 3 relevant vectors along with their metadata.
- Finally, it filters and returns results with a similarity score higher than 0.8, ensuring relevant product recommendations are provided to the user.

### Run the FastAPI Application (Locally)

Execute the following command in another terminal window to build and preview your FastAPI application:

```bash
uvicorn main:app --reload
```

The app should be running onÂ [localhost:8000](http://localhost:8000/).

Now, let's move on to creating the user interface for users to interact with.

## Create a new Astro application

Letâ€™s get started by creating a new Astro project. Open your terminal and run the following command:

```bash
npm create astro@latest ecommerce-vector-search-ui
```

`npm create astro` is the recommended way to scaffold an Astro project quickly.

When prompted, choose the following:

- `Empty` when prompted on how to start the new project.
- `Yes` when prompted whether to write Typescript.
- `Strict` when prompted how strict Typescript should be.
- `Yes` when prompted to whether install dependencies.
- `Yes` when prompted to whether initialize a git repository.

Once thatâ€™s done, you can move into the project directory and start the app:

```bash
cd ecommerce-vector-search-ui
npm run dev
```

The app should be running on [localhost:4321](http://localhost:4321/). Let's close the development server as we move on to integrate TailwindCSS into the application.

### Add Tailwind CSS to the application

For styling the app, you will be using Tailwind CSS. Install and set up Tailwind CSS at the root of our project's directory by running:

```bash
npx astro add tailwind
```

When prompted, choose:

- `Yes` when prompted to install the Tailwind dependencies.
- `Yes` when prompted to generate a minimal `tailwind.config.mjs` file.
- `Yes` when prompted to make changes to Astro configuration file.

With choices as above, the command finishes integrating TailwindCSS into your Astro project. It installed the following dependency:

- `tailwindcss`: TailwindCSS as a package to scan your project files to generate corresponding styles.
- `@astrojs/tailwind`: The adapter that brings Tailwind's utility CSS classes to every `.astro` file and framework component in your project.

To create reactive interfaces quickly, letâ€™s move onto integrating React in your Astro application.

### Integrate React in your Astro project

To prototype the reactive user interface quickly, you are gonna use React as the library with Astro. In your terminal window, execute the following command:

```bash
npx astro add react
```

`npx`Â allows us to execute npm packages binaries without having to first install it globally.

When prompted, choose the following:

- `Yes` when prompted whether to install the React dependencies.
- `Yes` when prompted whether to make changes to Astro configuration file.
- `Yes` when prompted whether to make changes to `tsconfig.json` file.

To create conversation user interface with product recommendations easily in React, letâ€™s move on to installing an AI SDK in your Astro application.

### Install Vercel AI SDK

In your terminal window, run the command below to install the necessary library for building the conversation user interface:

```bash
npm install ai
```

The above command installs the following:

- `ai` library to build AI-powered streaming text and chat UIs.

### Build Chat (with Product Recommendations) User Interface

TODO

### Test the Astro Application (Locally)

Execute the following command in another terminal window to build and preview your Astro application:

```bash
npm run build && npm run preview
```

The app should be running onÂ [localhost:4321](http://localhost:4321/).

> That was a lot of learning! Youâ€™re all done now âœ¨

## More Information

For more detailed insights, explore the references cited in this post.

- [GitHub Repo](https://github.com/rishi-raj-jain/schedule-qstash-queue-upstash)
- [Twitter OAuth 2.0 Flow](https://developer.twitter.com/en/docs/authentication/oauth-2-0/authorization-code)
- [Generating Twitter App Tokens](https://developer.twitter.com/en/docs/authentication/oauth-2-0/bearer-tokens)
- [React Form Hooks](https://react.dev/reference/react-dom/hooks)
- [Next.js Server Actions](https://nextjs.org/docs/app/building-your-application/data-fetching/server-actions-and-mutations)

## Conclusion

In this guide, you learned how to build a robust Tweet Scheduler leveraging Upstash's powerful Redis database and QStash queue. Upstash's scalability ensures reliable storage and scheduling of tweets combined with seamless deployment on Vercel create a fully automated system.
