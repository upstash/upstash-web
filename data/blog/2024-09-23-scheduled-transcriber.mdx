---
title: "Scheduling Audio Transcriptions with QStash"
slug: scheduled-transcriber
authors:
  - rishi
tags: [sdk, fireworks, clerk, ai, vercel]
---

In this post, you will learn how to build your scheduled transcriber with Upstash Qstash, Upstash Redis, Fireworks Transcription API, Clerk, Next.js App Router, and Vercel. TODO

## Prerequisites

You will need the following:

- [Node.js 18](https://nodejs.org/en/blog/announcements/v18-release-announce) or later
- A [Clerk](https://dashboard.clerk.com) account
- An¬†[Upstash](https://console.upstash.com)¬†account
- A [Fireworks](https://fireworks.ai) account
- A [Cloudflare](https://cloudflare.com) account
- A¬†[Vercel](https://vercel.com/dashboard)¬†account

## Tech Stack

| Technology | Description |
| --- | --- |
| [Next.js](https://nextjs.org) | The React Framework for the Web. |
| [Clerk](https://clerk.com) | User Management Platform. You are going to use it to add authentication to your application. |
| [Upstash](https://upstash.com) | Serverless database platform. You are going to use both Upstash QStash and Upstash Redis for scheduling transcriptions, and per-user transcription(s) status. |
| [Fireworks](https://fireworks.ai) | A generative AI inference platform to run and customize models with speed and production-readiness. |
| [Cloudflare R2](https://cloudflare.com) | A cloud object storage service. |
| [Vercel](https://vercel.com) | A cloud platform for deploying and scaling web applications. |

## Generate a Fireworks AI Token

Using Fireworks AI API, you are able to create transcription of an audio using AI. Any request to Fireworks AI API requires an authorization token. To obtain the token, navigate to the [API Keys](https://fireworks.ai/account/api-keys) in your Fireworks AI account, and click the **Create API Key** button. Copy and securely store this token for later use as **FIREWORKS_API_KEY** environment variable.

## Setting up Upstash Redis

In your Upstash dashboard, go to **Redis** tab and create a database.

![Create An Upstash Redis Instance](/blog/scheduled-transcriber/redis-create.png)

Scroll down until you find the REST API section, and select the `.env` button. Copy the content and save it somewhere safe.

![Upstash Redis Environment Variables](/blog/scheduled-transcriber/redis-env.png)

## Setting up Upstash QStash

To schedule POST requests to the endpoint transcribing an audio at a given interval, you will use QStash. Go to the **QStash** tab and scroll down to the **Request Builder** tab.

![Upstash QStash Tab](/blog/scheduled-transcriber/qstash-env.png)

Now, copy the QStash URL, QStash TOKEN, Current Signing Key, Next Signing Key, and save them somewhere safe.

## Create a new Clerk application

In your [Clerk Dashboard](https://dashboard.clerk.com/), to create a new app, press the¬†**+** **New application**¬†card to interactively start curating your own authentication setup form.

![Create a Clerk application](/blog/scheduled-transcriber/clerk-create.png)

With an application name of your choice, enable user authentication via credentials by toggling on **Email** and allow user authentication via Social Sign-On by toggling on providers such as **Google**, **GitHub** and **Microsoft**.

![Choose social logins](/blog/scheduled-transcriber/clerk-socials.png)

Once the application is created in the Clerk dashboard, you will be shown with your application's API keys for Next.js. Copy the content and save it somewhere safe.

![Clerk Environment Variables](/blog/scheduled-transcriber/clerk-env.png)

## Create a new Next.js application

Let‚Äôs get started by creating a new Next.js project. Open your terminal and run the following command:

```bash
npx create-next-app@latest my-app
```

When prompted, choose:

- `Yes` when prompted to use TypeScript.
- `No` when prompted to use ESLint.
- `Yes` when prompted to use Tailwind CSS.
- `No` when prompted to use `src/` directory.
- `Yes` when prompted to use App Router.
- `No` when prompted to customize the default import alias (`@/*`).

Once that is done, move into the project directory and start the app in developement mode by executing the following command:

```bash
cd my-app
npm run dev
```

The app should be running on [localhost:3000](http://localhost:3000). Stop the development server to install the necessary dependencies with the following commands:

```bash
npm install form-data node-fetch
npm install @clerk/nextjs
npm install @upstash/qstash @upstash/redis
npm install @aws-sdk/client-s3 @aws-sdk/s3-request-presigner
```

The libraries installed include:

- `form-data`: A library to create readable `multipart/form-data` streams.
- `node-fetch`: A module that brings the Fetch API to Node.js.
- `@clerk/nextjs`: Clerk‚Äôs SDK for Next.js.
- `@upstash/redis`: SDK to interact over HTTP requests with Redis, built on top of Upstash REST API.
- `@upstash/qstash`: SDK to interact with your Upstash QStash instance over HTTP requests.
- `@aws-sdk/client-s3`: AWS SDK for JavaScript S3 Client for Node.js, Browser and React Native.
- `@aws-sdk/s3-request-presigner`: SDK to generate a presigner based on signature V4 that will attempt to generate signed url for S3.

Now, create a `.env` file at the root of your project. You are going to add the `FIREWORKS_API_KEY`, `AWS_KEY_ID`, `AWS_REGION_NAME`, `AWS_S3_BUCKET_NAME`, `AWS_SECRET_ACCESS_KEY`, `CLOUDFLARE_R2_ACCOUNT_ID`, `NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY`, `CLERK_SECRET_KEY`, `QSTASH_TOKEN`, `QSTASH_CURRENT_SIGNING_KEY`, `QSTASH_NEXT_SIGNING_KEY`, `UPSTASH_REDIS_REST_URL`, and `UPSTASH_REDIS_REST_TOKEN` values you obtained earlier. It should look something like this:

```bash
# .env

# Fireworks Environment Variable
FIREWORKS_API_KEY="sk-None-..."

# Clerk Environment Variables
CLERK_SECRET_KEY="sk_test_..."
NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY="pk_test_..."

# Upstash Redis Environment Variables
UPSTASH_REDIS_REST_URL="https://...upstash.io"
UPSTASH_REDIS_REST_TOKEN="..."

# Upstash Qstash Environment Variables
QSTASH_TOKEN="..."
QSTASH_CURRENT_SIGNING_KEY="sig_..."
QSTASH_NEXT_SIGNING_KEY="sig_..."

# AWS Environment Variables
AWS_KEY_ID="..."
AWS_REGION_NAME="auto"
AWS_S3_BUCKET_NAME="..."
AWS_SECRET_ACCESS_KEY="..."
CLOUDFLARE_R2_ACCOUNT_ID="..."

# A unique/random seperator
RANDOM_SEPERATOR="4444Kdav"
```

To create API endpoints in Next.js, you will use Next.js [Route Handlers](https://nextjs.org/docs/app/building-your-application/routing/route-handlers) which allow you to serve responses over Web [Request](https://developer.mozilla.org/docs/Web/API/Request) and [Response](https://developer.mozilla.org/en-US/docs/Web/API/Response) APIs. To start creating API routes in Next.js that streams responses to the user, execute the following commands:

```bash
mkdir lib
mkdir -p app/api/get
mkdir -p app/api/upload
mkdir -p app/api/history
mkdir -p app/api/schedule
mkdir -p app/api/transcribe
```

<Note type="tip">
  The `-p` flag creates parent directories of a directory if they're missing.
</Note>

This sets up our Next.js project. Now, let's set up Clerk in the application.

## Set up Clerk SDK with Next.js

Clerk has a¬†[Next.js SDK](https://clerk.com/docs/references/nextjs/overview)¬†that contains helpers to make implementation of sign in modal, and managing (authenticated) sessions easier. You will add the¬†`ClerkProvider` component to the global layout of your Next.js application. This is a critical component as it provides access to the active session, and user context to all of Clerk‚Äôs components present anywhere in the application.

Make the following additions in `app/layout.tsx` to wrap the whole Next.js application with `ClerkProvider` component:

```diff
// File: app/layout.tsx

import './globals.css';
import type { Metadata } from 'next';
import { Inter } from 'next/font/google';
+ import { ClerkProvider } from '@clerk/nextjs';

const inter = Inter({ subsets: ["latin"] });

export const metadata: Metadata = {
  title: "Create Next App",
  description: "Generated by create next app",
};

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
      <html lang="en">
        <body className={inter.className}>
+      		<ClerkProvider>
              {children}
+      		</ClerkProvider>
        </body>
      </html>
  );
}
```

Now, let's move on to configuring the Next.js middleware for managing sessions with Clerk.

## Configure Next.js Middleware for Clerk

Clerk¬†requires middleware¬†to allow granular control of protection via authentication over routes (including router handlers) on a per-request basis. Create a¬†`middleware.ts`¬†file at the root of your project with the following:

```tsx
// File: middleware.ts

import { clerkMiddleware } from '@clerk/nextjs/server'

export default clerkMiddleware()

export const config = {
  matcher: ['/((?!.*\\..*|_next).*)'],
}
```

The code above imports Clerk's¬†[`clerkMiddleware`](https://clerk.com/docs/references/nextjs/clerk-middleware)¬†helper extending the ability to mark specific routes as public (i.e. they are accessible without authentication), as ignored (i.e. authentication checks are not ran on such pages), and as API Routes (i.e. they are treated by Clerk as API Endpoints). The middleware is applied to the route paths matching the `matcher` option in `config` object, which per the above config is **all non-static assets** paths.

Now, let's integrate `shadcn/ui` components in Next.js.

## Integrating shadcn/ui components

To quickly prototype the user interface, you will set up the `shadcn/ui` with Next.js. `shadcn/ui` is a collection of beautifully designed components that you can copy and paste into your apps. To set up `shadcn/ui`, execute the command below:

```bash
npx shadcn-ui@latest init
```

You will be asked a few questions to configure a¬†`components.json`, choose the following:

- `Yes` when prompted to use TypeScript.
- `Slate` when prompted to choose the base color.
- `yes` when prompted to use CSS variables for colors.
- `Yes` when prompted to proceed with writing the configuration to components.json.

Once that is done, you have set up a CLI that allows us to easily add React components to your Next.js application. Next, execute the command below to get the button, input, tooltip, and toast elements:

```bash
npx shadcn-ui@latest add button
npx shadcn-ui@latest add input
npx shadcn-ui@latest add toast
npx shadcn-ui@latest add tooltip
```

Once that is done, you would now see a `ui` directory inside the `app/components` directory containing `button.tsx`, `input.tsx`, `tooltip.tsx`, `toast.tsx`, `toaster.tsx`, and `use-toast.ts`.

Next, open up the `app/layout.tsx` file, and make the following additions:

```diff
// File: app/layout.tsx

import './globals.css';
import type { Metadata } from 'next';
import { Inter } from 'next/font/google';
import { ClerkProvider } from '@clerk/nextjs';
+ import { Toaster } from '@/components/ui/toaster';

const inter = Inter({ subsets: ["latin"] });

export const metadata: Metadata = {
  title: "Create Next App",
  description: "Generated by create next app",
};

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
      <html lang="en">
        <body className={inter.className}>
      		<ClerkProvider>
              {children}
      		</ClerkProvider>
+           <Toaster />
        </body>
      </html>
  );
}
```

In the code changes above, you have imported the `Toaster` component, and made sure that it is present in your entire Next.js application. It enables you to show toast notifications from anywhere in your code via the `useToast` React hook.

Now, let's move on to implementing the transcription history API endpoint.

## Transcriptions History API Endpoint in Next.js App Router

Create a file named `route.ts` in the `app/api/history` directory that retrieves all the transcriptions done for a particular user in a paginated form, with the following code:

```tsx
// File: app/api/history/route.ts

export const runtime = 'nodejs'

export const dynamic = 'force-dynamic'

export const fetchCache = 'force-no-store'

import redis from '@/lib/redis.server'
import { auth } from '@clerk/nextjs/server'
import { NextResponse, type NextRequest } from 'next/server'

export async function GET(request: NextRequest) {
  const { userId } = auth()
  if (!userId) return new Response(null, { status: 403 })
  const count = 10
  const audioNames = []
  const searchParams = request.nextUrl.searchParams
  const start = parseInt(searchParams.get('start') ?? '0')
  const [_, items] = await redis.hscan(userId, start, { count })
  for (let i = 0; i < items.length; i += 2) audioNames.push({ key: items[i], value: items[i + 1] })
  return NextResponse.json(audioNames)
}
```

The Next.js API endpoint retrieves the transcription history for authenticated users by checking their authentication status with Clerk. It uses Redis to fetch the data in a paginated format and returns the transcription history as a JSON response, allowing clients to specify a starting point for retrieval.

Now, let's move on to implementing the audio transcription API endpoint.

## Transcribe Audio API Endpoint in Next.js App Router

Create a file named `route.ts` in the `app/api/transcribe` directory that transcribes an audio after fetching it from Cloudflare R2, with the following code:

```tsx
// File: app/api/transcribe/route.ts

export const runtime = 'nodejs'

export const dynamic = 'force-dynamic'

export const fetchCache = 'force-no-store'

import redis from '@/lib/redis.server'
import { getS3Object } from '@/lib/storage.server'
import { verifySignatureAppRouter } from '@upstash/qstash/dist/nextjs'
import FormData from 'form-data'
import fetch from 'node-fetch'

export const POST = verifySignatureAppRouter(handler)

async function handler(request: Request) {
  const { fileName } = await request.json()
  const url = await getS3Object(fileName)
  const response = await fetch(url)
  if (!response.ok) throw new Error(`Failed to fetch audio file: ${response.statusText}.`)
  const arrayBuffer = await response.arrayBuffer()
  const buffer = Buffer.from(arrayBuffer)
  const form = new FormData()
  form.append('file', buffer)
  form.append('language', 'en')
  const options = {
    body: form,
    method: 'POST',
    headers: {
      Authorization: `Bearer ${process.env.FIREWORKS_API_KEY}`,
    },
  }
  const transcribeCall = await fetch('https://api.fireworks.ai/inference/v1/audio/transcriptions', options)
  const transcribeResp: any = await transcribeCall.json()
  if (transcribeResp?.['text']) {
    await redis.hset(fileName.split(process.env.RANDOM_SEPERATOR)[0], {
      [fileName.split(process.env.RANDOM_SEPERATOR)[1]]: {
        transcribed: true,
        transcription: transcribeResp.text,
      },
    })
  }
  return new Response()
}
```

The code above defines a Next.js API endpoint for transcribing audio files using the Fireworks AI service. It utilizes the `verifySignatureAppRouter` middleware from Upstash QStash to ensure secure requests. The handler fetches the audio file from Cloudflare R2, processes it into a `FormData` object, and sends it to the Fireworks API for transcription. Upon receiving the transcription response, it stores the results in Upstash Redis, associating them with the user's unique identifier.

## Building the chatbot interface

To start building the application's user interface, open the `app/page.tsx` file and replace the existing code with the following:

```tsx
// File: app/page.tsx

'use client'

import { useChat } from 'ai/react'
import { useUser } from '@clerk/nextjs'
import { useEffect, useState } from 'react'
import { useToast } from '@/components/ui/use-toast'

export default function Page() {
  const { toast } = useToast()
  const { isSignedIn } = useUser()
  const [disabled, setDisabled] = useState(true)
  const { messages, input, handleInputChange, handleSubmit, setMessages } = useChat()
  useEffect(() => {
    if (isSignedIn) {
      fetch('/api/chat/history')
        .then((res) => res.json())
        .then((res) => {
          if (res?.messages?.length > 0) setMessages(res.messages)
        })
        .finally(() => setDisabled(false))
    }
  }, [isSignedIn])
  return (
    <div className="mx-auto flex w-full max-w-md flex-col py-8">
     {/* Rest of components */}
    </div>
  )
}
```

The code above begins with importing the React hooks for managing the state of the user, the conversation, and invoke toast elements. As the component is mounted (and if the user's logged in state changes), the chat history for the user is fetched.

Now, perform the following additions to be able to add user-uploaded PDF(s) to their vector index namespace:

```diff
// File: app/page.tsx

'use client'

import { useChat } from 'ai/react'
import { useUser } from '@clerk/nextjs'
import { useEffect, useState } from 'react'
import { useToast } from '@/components/ui/use-toast'

export default function Page() {
  // useState and useEffect code
  return (
    <div className="mx-auto flex w-full max-w-md flex-col py-8">
+      <input
+          type="file"
+          id="fileInput"
+          className="hidden"
+          accept="application/pdf"
+          onChange={() => {
+            const fileInput = document.getElementById('fileInput') as HTMLInputElement
+            if (!fileInput || !fileInput.files || fileInput.files.length === 0) {
+              toast({
+                duration: 2000,
+                variant: 'destructive',
+                description: 'No file attached.',
+              })
+              return
+            }
+            const fileData = fileInput.files[0]
+            const formData = new FormData()
+            formData.append('file', fileData)
+            const loadingToast = toast({
+              duration: 10000,
+              description: "Adding your PDF to AI's knowledge...",
+            })
+            fetch('/api/upsert', {
+              method: 'POST',
+              body: formData,
+            }).then((res) => {
+              loadingToast.dismiss()
+              if (res.ok) {
+                toast({
+                  duration: 2000,
+                  description: "Added the PDF to AI's knowledge succesfully.",
+                })
+              } else {
+                toast({
+                  duration: 2000,
+                  variant: 'destructive',
+                  description: "Failed to add the PDF to AI's knowledge.",
+                })
+              }
+            })
+          }}
+        />
     {/* Rest of components */}
    </div>
  )
}
```

The code above defines an `input` element that only accepts a pdf file from the user. When a PDF is uploaded, via the `onChange` callback, it `POST`s the file to `/api/upsert` api endpoint. Once that's done, it displays a toast indiciating if the operation was a success.

Now, perform the following additions to switch states depending on user authentication status:

```diff
// File: app/page.tsx

'use client'

import { useChat } from 'ai/react'
import { useEffect, useState } from 'react'
import { useToast } from '@/components/ui/use-toast'
- import { useUser } from '@clerk/nextjs'
+ import MemoizedMD from '@/components/memoized-react-markdown'
+ import { SignInButton, SignedIn, SignedOut, UserButton, useUser } from '@clerk/nextjs'

export default function Page() {
  // useState and useEffect code
  return (
    <div className="mx-auto flex w-full max-w-md flex-col py-8">
      {/* Input component */}
+      <div className="flex flex-row items-start justify-between">
+        <span className="text-xl font-semibold">NerdCoach</span>
+        <SignedIn>
+          <div className="size-[28px] rounded-full bg-black/10">
+            <UserButton />
+          </div>
+        </SignedIn>
+      </div>
+      {isSignedIn ? (
+        disabled ? (
+          <div className="mt-8 flex flex-col gap-y-2">
+            <div className="h-[30px] animate-pulse bg-black/10" />
+            <div className="h-[30px] animate-pulse bg-black/10" />
+            <div className="h-[30px] animate-pulse bg-black/10" />
+          </div>
+        ) : (
+          messages.map(({ content }, idx) => <MemoizedMD key={idx} message={content} />)
+        )
+      ) : (
+        <div className="mt-8 flex max-w-max flex-col justify-center">
+          <SignedOut>
+            <div className="rounded border px-3 py-1 shadow transition duration-300 hover:shadow-md">
+              <SignInButton mode="modal">Sign in to use NerdCoach &rarr;</SignInButton>
+            </div>
+          </SignedOut>
+        </div>
+      )}
     {/* Rest of components */}
    </div>
  )
}
```

The code above creates a signed in and out state of the application for the user. If the user is signed in, it displays their image via `UserButton` component with additional actions, and a default loading state of their chat history. If the user is signed out, it shows a `Sign in to use Nerdcoach` button.

Now, perform the following additions to add an input for user to submit their prompt and upload PDF file:

```diff
// File: app/page.tsx

'use client'

import { useChat } from 'ai/react'
import { useEffect, useState } from 'react'
import { useToast } from '@/components/ui/use-toast'
import MemoizedMD from '@/components/memoized-react-markdown'
import { SignInButton, SignedIn, SignedOut, UserButton, useUser } from '@clerk/nextjs'
+ import { Upload } from 'lucide-react'
+ import { Input } from '@/components/ui/input'
+ import { Tooltip, TooltipContent, TooltipProvider, TooltipTrigger } from '@/components/ui/tooltip'

export default function Page() {
  // useState and useEffect code
  return (
    <div className="mx-auto flex w-full max-w-md flex-col py-8">
      {/* Input component */}
      {/* Rest of components */}
+      <div className="fixed bottom-0 mb-8 flex w-full max-w-[82vw] flex-row items-center shadow sm:max-w-md">
+        <div className="cursor-pointer border bg-white px-2 py-1 pt-2 text-gray-400 hover:text-gray-800">
+          <TooltipProvider>
+            <Tooltip>
+              <TooltipTrigger
+                onClick={() => {
+                  const tmp = document.querySelector(`[id="fileInput"]`) as HTMLInputElement
+                  tmp?.click()
+                }}
+              >
+                <Upload className="size-[20px]" />
+              </TooltipTrigger>
+              <TooltipContent>
+                <span>Upload Resume</span>
+              </TooltipContent>
+            </Tooltip>
+          </TooltipProvider>
+        </div>
+        <Input
+          value={input}
+          disabled={disabled}
+          className="!rounded-none"
+          onChange={handleInputChange}
+          placeholder="Ask something..."
+          onKeyDown={(e) => {
+            if (e.key.toLowerCase() == 'enter') handleSubmit()
+          }}
+        />
+      </div>
    </div>
  )
}
```

The code above defines an `input` element which after the user submits their prompt, calls the `/api/chat` endpoint. It also defines a `TooltipProvider` component which invokes the PDF file uploader `input` if a user clicks the upload icon.

> That was a lot of learning! You‚Äôre all done now ‚ú®

## Deploy to Vercel

The repository, is now ready to deploy to Vercel. Use the following steps to deploy üëáüèª

- Start by creating a GitHub repository containing your app's code.
- Then, navigate to the Vercel Dashboard and create a¬†**New Project**.
- Link the new project to the GitHub repository you just created.
- In¬†**Settings**, update the¬†`Environment Variables`¬†to match those in your local¬†`.env`¬†file.
- Deploy! üöÄ

## More Information

For more detailed insights, explore the references cited in this post.

- [GitHub Repository](https://github.com/upstash/transcriber)
- [Upstash Qstash](https://upstash.com/docs/qstash/features/schedules)
- [Fireworks Transcribe Audio API](https://docs.fireworks.ai/api-reference/audio-transcriptions)
- [Next.js Streaming](https://vercel.com/docs/functions/streaming/quickstart)

## Conclusion

(TODO) In this blog, you learned how to build a career coach application using Upstash Vector, Upstash Redis, OpenAI API, Clerk, Next.js App Router, and Vercel. You set up Upstash to manage vector embeddings and chat history, configured Clerk for user authentication, and integrated OpenAI for AI-powered responses. The blog walked you through creating API endpoints for chat interactions, adding context from user-uploaded PDFs, and building a user interface with authentication and file upload functionalities.
