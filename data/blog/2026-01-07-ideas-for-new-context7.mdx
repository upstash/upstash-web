---
title: "Ideas for a new Context7"
slug: ideas-for-new-context7
authors: [josh, enes]
tags: [context7]
draft: true
---

Right now, Context7 works like this: when an AI makes a request through MCP, it's a two-step process. The first call resolves a library name to a Context7 ID. The second call fetches the actual documentation from our vector database:

<Frame>
  <img src="/blog/ideas-for-new-context7/old-infra.png" />
</Frame>

**The problem:** The second call, responsible for getting documentation, can return a lot of results. Sometimes more than necessary. When all those docs get packed into the context window of an LLM, we end up with more expensive generations and potentially worse output (this is also called "context bloat").

<Frame>
  <img src="/blog/ideas-for-new-context7/context-bloat.png" />
</Frame>

**We think we can do better.** So now, we're running an experiment to move the filtering and ranking work to our side. Instead of returning everything the vector database finds and letting your AI model figure out what's useful, we're experimenting with high-quality reranking on the server using fast SOTA models in the Gemini series.

That way, the model (should ü§û) only gets back the specific documentation pieces that answer the query. The only trade-off here is that a query takes slightly longer. From an infra perspective, imagine it like this:

<Frame>
  <img src="/blog/ideas-for-new-context7/new-infra.png" />
</Frame>

Plus, it's really cool to see that people are as optimistic as we are! üëá

<Frame>
  <img src="/blog/ideas-for-new-context7/optimistic.png" />
</Frame>

Instead of handing back everything the vector database finds, we want to:

- Make each MCP request take one call instead of two
- Send back less documentation, only the most relevant pieces
- Thereby consume (much) less context

This is a general infrastructure change, but let's use our SDK to illustrate the changes.

---

## Idea: Search by question, not library name

As illustrated above, right now the first call resolves a library name to a Context7 ID. The second call fetches the actual documentation from our vector database. It looks like this:

```typescript {6,10}
import { Context7 } from "@upstash/context7-sdk";

const client = new Context7();

// 1Ô∏è‚É£ - search indexed libraries
const { results } = await client.searchLibrary("react");
const library = results.at(0);

// 2Ô∏è‚É£ - query library
const docs = await client.getDocs(library.id, {
  mode: "code",
  topic: "hooks",
  limit: 10,
  page: 1,
});
```

We want to replace that with something like `getContext()`. Instead of managing pagination, modes, and topics, the LLM just asks a question and gets back only the documentation it's looking for:

```typescript {6}
import { Context7 } from "@upstash/context7-sdk";

const client = new Context7();

// New approach - one step with reranking on the server
const context = await client.getContext(
  "How do I use hooks?",
  "/facebook/react",
);
```

By default you'd get back a string of the most relevant docs concatenated together. If you need structured data, you can ask for JSON, too. The server-side reranker takes care of the complexity that was caused by `mode` and `pagination` before.

We'll probably also drop `defaultMaxResults`. The API should return what's relevant without manual tuning.

---

## Why We're Doing This

Judging from feedback, especially on Twitter, context bloat is the number 1 issue that holds people back from using Context7. Especially with the explosion of Opus 4.5, an extremely good but expensive model, it's more than enough to use a single or very few pieces of accurate documentation.

By reranking on our side, we move that filtering cost from the user (who likely uses an expensive model) to us (where we can use more cost-efficient models because re-ranking is fairly simple & fast).

As a nice bonus, the MCP also becomes simpler. Instead of managing pagination and modes, the LLM just asks a question and gets an answer.

---

## What's Next

We're testing the new architecture internally right now. This is not final by any means, but it's an experiment we're running to see if the tradeoffs make sense. We want to be upfront about where we're heading, and we'd genuinely like your input if you're using Context7.

If this approach solves something you've been frustrated with, or if it creates a problem for your use case, let me know!

Appreciate you for reading üôå
