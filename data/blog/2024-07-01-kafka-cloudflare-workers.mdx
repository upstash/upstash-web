---
slug: implementing-upstash-kafka-with-cloudflare-workers
title: "Handling Billions of LLM Logs with Upstash Kafka and Cloudflare Workers"
authors:
  - cole
tags:
  [
    kafka,
    upstash,
    cloudflare workers,
    serverless,
    llm observability,
    log processing,
    scalable architecture,
    event streaming,
    ai infrastructure,
    distributed systems,
    batch processing,
  ]
---

## Problem

![kafka.png](/blog/cloudflare-workers-kafka/kafka.png)

> **Important:** We desperately needed a solution to these outages/data loss. Our reliability and scalability are core to our product.

## Introduction

At [Helicone](https://helicone.ai), an open-source LLM observability platform, we faced significant challenges in scaling our logging infrastructure to match our growing user base. Our proxy, built on [Cloudflare Workers](https://developers.cloudflare.com/workers/), efficiently handled routing LLM requests, but our logging system struggled with high-volume data processing. This technical deep dive explores how we implemented [Upstash Kafka](https://upstash.com/docs/kafka/overall/getstarted) to solve these challenges, detailing our architecture, implementation process, and key technical decisions.

Whether you're currently handling millions of logs and anticipating growth, or already dealing with billions of events, the insights shared here will provide a real-world example of scaling a logging infrastructure for LLM applications. We'll cover our experience with serverless architecture using Cloudflare Workers and how we integrated it with Kafka for efficient log processing at scale.

### The Initial Architecture and Its Limitations

Let's start by examining our initial serverless architecture and its limitations. It worked as follows:

1. A client initiates an LLM request, which is proxied through Heliconeâs Cloudflare workers to the desired LLM provider.
2. After receiving the providerâs response and returning it to the client, we process the log with our business logic and insert it into databases.

![architecture1.png](/blog/cloudflare-workers-kafka/architecture1.png)

### **Why Didn't Our Initial Setup Scale?**

Our initial architecture faced several significant challenges:

1. **Inefficient Event Processing**: We were processing logs one at a time, a method that doesn't scale well with high-volume data.
2. **Data Loss During Downtime**: Any service interruption meant lost logs and important data.
3. **Limited Reprocessing**: When bugs caused incorrectly processed logs, we lacked robust means to reprocess them.
4. **Cloudflare Worker [Limits](https://developers.cloudflare.com/workers/platform/limits/)**: Cloudflare Workers have strict limits on memory, CPU, and execution time for **`event.waitUntil`**, making it hard to handle growing log volumes.

All these issues resulted in frequent downtimes and lost logs, especially as our traffic surged. We urgently needed a scalable and reliable solution!

## **Introducing Upstash Kafka**

### **Realizing the Need for a Persistent Queue**

Increasing traffic and logging challenges highlighted the need for a persistent queue. Persistent queues decouple logging from our proxy, preventing data loss during downtimes, enabling batch log processing, and buffering traffic spikes to avoid database overload.

#### **Why [Kafka](https://kafka.apache.org/)?**

After evaluating several queue solutions, we chose Kafka for its unique advantages in high-volume data streaming:

- **High Throughput**: Kafka efficiently handles millions of messages per second, far surpassing traditional messaging queues.
- **Persistence**: Unlike many message queues, Kafka stores data on disk, allowing for replay and longer retention of messages.
- **Distributed System**: Kafka's architecture of topics and partitions allows for easy scaling and parallel processing.

#### Choosing Upstash Kafka

We compared Kafka with other solutions based on our core requirements:

| Criteria                    | [AWS MSK](https://aws.amazon.com/msk/) | [Redpanda Cloud](https://redpanda.com/redpanda-cloud) | [Upstash Kafka](https://upstash.com/docs/kafka/overall/getstarted) |
| --------------------------- | -------------------------------------- | ----------------------------------------------------- | ------------------------------------------------------------------ |
| Managed Service             | â                                      | â                                                     | â                                                                  |
| HTTP Endpoint               | â                                      | â                                                     | â                                                                  |
| Quick Setup and Integration | â                                      | â                                                     | â                                                                  |
| Reasonable Pricing          | â                                      | â                                                     | â                                                                  |
| Excellent Support           | â                                      | â                                                     | â                                                                  |

Upstash Kafka offers a fully managed Kafka cluster with an HTTP endpoint, perfect for our Cloudflare Workers serverless architecture. Its quick setup, reasonable pricing, and excellent support made it the best choice.

> **Note**: While we chose Upstash Kafka for our implementation, the principles and architecture discussed in this blog are applicable to other Kafka deployments and managed Kafka services.

## Implementing Kafka

Here's a sequence diagram illustrating the new data flow:

1. A client initiates an LLM request, which is proxied through Heliconeâs proxy to the desired LLM provider.
2. After receiving the provider's response and sending it to the client, we store the raw request/response bodies in S3. We then publish the rest directly to Kafka before any processing.
3. Our ECS consumer service consumes batches of_logs from Kafka, processes them asynchronously, and inserts the entire batch in a single DB transaction.

![architecture2.png](/blog/cloudflare-workers-kafka/architecture2.png)

### Kafka Cluster Configuration

Upstash handles most of the configuration, like [Kafka Brokers](https://developer.confluent.io/courses/apache-kafka/brokers/) and [Kafka Replication Factors](https://docs.confluent.io/kafka/design/replication.html). For what we do control, here's our setup:

#### **Topics**

[Topics](https://developer.confluent.io/courses/apache-kafka/topics/) are used to organize and categorize messages in Kafka. Each topic corresponds to a specific type of data. For our use case involving LLM logs, we have two topics:

- `request-response-logs`: This topic stores logs of all LLM requests and responses.
  - Configuration: 30 partitions, 7-day retention, 1TB storage capacity.
- `request-response-dlq`: This topic serves as a dead-letter queue for logs that failed processing in the main logs topic.
  - Configuration: 15 partitions, 7-day retention, 1TB storage capacity.

These topics store messages with the following structure:

```typescript
// Message topics contain
export type KafkaMessage = {
  id: string;
  heliconeMeta: HeliconeMeta;
  log: Log;
};

// Actual log of LLM request/response
export type Log = {
  request: {
    id: string;
    userId: