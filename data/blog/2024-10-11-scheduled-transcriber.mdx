---
title: "Scheduling Audio Transcriptions with QStash"
slug: scheduled-transcriber
authors:
  - rishi
tags: [sdk, fireworks, clerk, ai, vercel]
---

In this tutorial, you will learn how to build a scheduled audio transcription system using Upstash QStash for task scheduling and Fireworks AI for AI-powered transcriptions. You will also learn techniques to secure file uploads to Cloudflare R2, authenticate users with Clerk, and store and fetch data from Upstash Redis.

With a focus on being able to scale, updating the state of the users, the history of transcriptions, and offloading the transcription work to background workers, the application heavily relies on QStash. The application will allow users to upload audio files that need to be transcribed, create a background process to schedule the transcription, update the history of transcriptions (for the particular user), and then display it back to them in the frontend.  

## Demo

Before jumping into the technical stuff, let me give you a sneak peek of what you will build in this tutorial.

<video src="https://github.com/user-attachments/assets/9d1484be-2a69-4659-8873-5f8a2f3eb8eb" controls></video>

## Prerequisites

You will need the following:

- [Node.js 18](https://nodejs.org/en/blog/announcements/v18-release-announce) or later
- A [Clerk](https://dashboard.clerk.com) account
- An [Upstash](https://console.upstash.com) account
- A [Fireworks](https://fireworks.ai) account
- A [Cloudflare](https://cloudflare.com) account
- A [Vercel](https://vercel.com/dashboard) account

## Tech Stack

| Technology | Description |
| --- | --- |
| [Next.js](https://nextjs.org) | The React Framework for the Web. |
| [Clerk](https://clerk.com) | User Management Platform. You are going to use it to add authentication to your application. |
| [Upstash](https://upstash.com) | Serverless database platform. You are going to use both Upstash QStash and Upstash Redis for scheduling transcriptions, and per-user transcription(s) status. |
| [Fireworks](https://fireworks.ai) | A generative AI inference platform to run and customize models with speed and production-readiness. |
| [Cloudflare R2](https://cloudflare.com) | A cloud object storage service. |
| [Vercel](https://vercel.com) | A cloud platform for deploying and scaling web applications. |

## Generate a Fireworks AI Token

Using the Fireworks AI API, you are able to create a transcription of an audio using AI. Any request to the Fireworks AI API requires an authorization token. To obtain the token, navigate to the [API Keys](https://fireworks.ai/account/api-keys) in your Fireworks AI account, and click the **Create API Key** button. Copy and securely store this token for later use as **FIREWORKS_API_KEY** environment variable.

## Setting up Upstash Redis

In your Upstash dashboard, go to the **Redis** tab and create a database.

![Create An Upstash Redis Instance](/blog/scheduled-transcriber/redis-create.png)

Scroll down until you find the REST API section, and select the `.env` button. Copy the content and save it somewhere safe.

![Upstash Redis Environment Variables](/blog/scheduled-transcriber/redis-env.png)

## Setting up Upstash QStash

To schedule POST requests to the endpoint transcribing an audio at a given interval, you will use QStash. Go to the **QStash** tab and scroll down to the **Request Builder** tab.

![Upstash QStash Tab](/blog/scheduled-transcriber/qstash-env.png)

Now, copy the QStash URL, QStash TOKEN, Current Signing Key, Next Signing Key, and save them somewhere safe.

## Create a new Clerk application

In your [Clerk Dashboard](https://dashboard.clerk.com/), to create a new app, press the **+ New application** card to interactively start curating your own authentication setup form.

![Create a Clerk application](/blog/scheduled-transcriber/clerk-create.png)

With an application name of your choice, enable user authentication via credentials by toggling on **Email** and allow user authentication via Social Sign-On by toggling on providers such as **Google**, **GitHub**, and **Microsoft**.

![Choose social logins](/blog/scheduled-transcriber/clerk-socials.png)

Once the application is created in the Clerk dashboard, you will be shown with your application's API keys for Next.js. Copy the content and save it somewhere safe.

![Clerk Environment Variables](/blog/scheduled-transcriber/clerk-env.png)

## Create a new Next.js application

Letâs get started by creating a new Next.js project. Open your terminal and run the following command:

```bash
npx create-next-app@latest my-app
```

When prompted, choose:

- `Yes` when prompted to use TypeScript.
- `No` when prompted to use ESLint.
- `Yes` when prompted to use Tailwind CSS.
- `No` when prompted to use `src/` directory.
- `Yes` when prompted to use App Router.
- `No` when prompted to customize the default import alias (`@/*`).

Once that is done, move into the project directory and start the app in development mode by executing the following command:

```bash
cd my-app
npm run dev
```

The app should be running on [localhost:3000](http://localhost:3000). Stop the development server to install the necessary dependencies with the following commands:

```bash
npm install form-data node-fetch
npm install @clerk/nextjs
npm install @upstash/qstash @upstash/redis
npm install @aws-sdk/client-s3 @aws-sdk/s3-request-presigner
```

The libraries installed include:

- `form-data`: A library to create readable `multipart/form-data` streams.
- `node-fetch`: A module that brings the Fetch API to Node.js.
- `@clerk/nextjs`: Clerkâs SDK for Next.js.
- `@upstash/redis`: SDK to interact over HTTP requests with Redis, built on top of Upstash REST API.
- `@upstash/qstash`: SDK to interact with your Upstash QStash instance over HTTP requests.
- `@aws-sdk/client-s3`: AWS SDK for JavaScript S3 Client for Node.js, Browser and React Native.
- `@aws-sdk/s3-request-presigner`: SDK to generate a presigner based on signature V4 that will attempt to generate signed url for S3.

Now, create a `.env` file at the root of your project. You are going to add the `FIREWORKS_API_KEY`, `AWS_KEY_ID`, `AWS_REGION_NAME`, `AWS_S3_BUCKET_NAME`, `AWS_SECRET_ACCESS_KEY`, `CLOUDFLARE_R2_ACCOUNT_ID`, `NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY`, `CLERK_SECRET_KEY`, `QSTASH_TOKEN`, `QSTASH_CURRENT_SIGNING_KEY`, `QSTASH_NEXT_SIGNING_KEY`, `UPSTASH_REDIS_REST_URL`, and `UPSTASH_REDIS_REST_TOKEN` values you obtained earlier. It should look something like this:

```bash
# .env

# Fireworks Environment Variable
FIREWORKS_API_KEY="sk-None-..."

# Clerk Environment Variables
CLERK_SECRET_KEY="sk_test_..."
NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY="pk_test_..."

# Upstash Redis Environment Variables
UPSTASH_REDIS_REST_URL="https://...upstash.io"
UPSTASH_REDIS_REST_TOKEN="..."

# Upstash Qstash Environment Variables
QSTASH_TOKEN="..."
QSTASH_CURRENT_SIGNING_KEY="sig_..."
QSTASH_NEXT_SIGNING_KEY="sig_..."

# AWS Environment Variables
AWS_KEY_ID="..."
AWS_REGION_NAME="auto"
AWS_S3_BUCKET_NAME="..."
AWS_SECRET_ACCESS_KEY="..."
CLOUDFLARE_R2_ACCOUNT_ID="..."

# A unique/random separator
RANDOM_SEPARATOR="4444Kdav"
```

To create API endpoints in Next.js, you will use Next.js [Route Handlers](https://nextjs.org/docs/app/building-your-application/routing/route-handlers) which allow you to serve responses over Web [Request](https://developer.mozilla.org/docs/Web/API/Request) and [Response](https://developer.mozilla.org/en-US/docs/Web/API/Response) APIs. To start creating API routes in Next.js that streams responses to the user, execute the following commands:

```bash
mkdir lib
mkdir -p app/api/get
mkdir -p app/api/upload
mkdir -p app/api/history
mkdir -p app/api/schedule
mkdir -p app/api/transcribe
```

<Note type="tip">
  The `-p` flag creates parent directories of a directory if they're missing.
</Note>

This sets up our Next.js project. Now, let's set up Clerk in the application.

## Set up Clerk SDK with Next.js

Clerk has a [Next.js SDK](https://clerk.com/docs/references/nextjs/overview) that contains helpers to make implementation of sign-in modal, and managing (authenticated) sessions easier. You will add the `ClerkProvider` component to the global layout of your Next.js application. This is a critical component as it provides access to the active session, and user context to all of Clerkâs components present anywhere in the application.

Make the following additions in `app/layout.tsx` to wrap the whole Next.js application with `ClerkProvider` component:

```diff
// File: app/layout.tsx

import './globals.css';
import type { Metadata } from 'next';
import { Inter } from 'next/font/google';
+ import { ClerkProvider } from '@clerk/nextjs';

const inter = Inter({ subsets: ["latin"] });

export const metadata: Metadata = {
  title: "Create Next App",
  description: "Generated by create next app",
};

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
      <html lang="en">
        <body className={inter.className}>
+      		<ClerkProvider>
              {children}
+      		</ClerkProvider>
        </body>
      </html>
  );
}
```

Now, let's move on to configuring the Next.js middleware for managing sessions with Clerk.

## Configure Next.js Middleware for Clerk

Clerk requires middleware to allow granular control of protection via authentication over routes (including router handlers) on a per-request basis. Create a `middleware.ts` file at the root of your project with the following:

```tsx
// File: middleware.ts

import { clerkMiddleware } from '@clerk/nextjs/server'

export default clerkMiddleware()

export the config = {
  matcher: ['/((?!.*\\..*|_next).*)'],
}
```

The code above imports Clerk's [`clerkMiddleware`](https://clerk.com/docs/references/nextjs/clerk-middleware) helper extending the ability to mark specific routes as public (i.e. they are accessible without authentication), as ignored (i.e. authentication checks are not run on such pages), and as API Routes (i.e. they are treated by Clerk as API Endpoints). The middleware is applied to the route paths matching the `matcher` option in the `config` object, which per the above config is **all non-static assets** paths.

Now, let's integrate `shadcn/ui` components in Next.js.

## Integrating shadcn/ui components

To quickly prototype the user interface, you will set up the `shadcn/ui` with Next.js. `shadcn/ui` is a collection of beautifully designed components that you can copy and paste into your apps. To set up `shadcn/ui`, execute the command below:

```bash
npx shadcn-ui@latest init
```

You will be asked a few questions to configure a `components.json`, choose the following:

- `Yes` when prompted to use TypeScript.
- `Slate` when prompted to choose the base color.
- `yes` when prompted to use CSS variables for colors.
- `Yes` when prompted to proceed with writing the configuration to components.json.

Once that is done, you have set up a CLI that allows us to easily add React components to your Next.js application. Next, execute the command below to get the button, input, tooltip, and toast elements:

```bash
npx shadcn-ui@latest add button
npx shadcn-ui@latest add input
npx shadcn-ui@latest add toast
npx shadcn-ui@latest add tooltip
```

Once that is done, you would now see a `ui` directory inside the `app/components` directory containing `button.tsx`, `input.tsx`, `tooltip.tsx`, `toast.tsx`, `toaster.tsx`, and `use-toast.ts`.

Next, open up the `app/layout.tsx` file, and make the following additions:

```diff
// File: app/layout.tsx

import './globals.css';
import type { Metadata } from 'next';
import the Inter from 'next/font/google';
import { ClerkProvider } from '@clerk/nextjs';
+ import { Toaster } from '@/components/ui/toaster';

the inter = Inter({ subsets: ["latin"] });

export the metadata: Metadata = {
  title: "Create Next App",
  description: "Generated by create next app",
};

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
      <html lang="en">
        <body className={inter.className}>
      		<ClerkProvider>
              {children}
      		</ClerkProvider>
+           <Toaster />
        </body>
      </html>
  );
}
```

In the code changes above, you have imported the `Toaster` component, and made sure that it is present in your entire Next.js application. It enables you to show toast notifications from anywhere in your code via the `useToast` React hook.

## Transcribe Audio API Endpoint in Next.js App Router

Create a file named `route.ts` in the `app/api/transcribe` directory that transcribes an audio after fetching it from Cloudflare R2, with the following code:

```tsx
// File: app/api/transcribe/route.ts

export the runtime = 'nodejs'

export the dynamic = 'force-dynamic'

export the fetchCache = 'force-no-store'

import redis from '@/lib/redis.server'
import the getS3Object from '@/lib/storage.server'
import the verifySignatureAppRouter from '@upstash/qstash/dist/nextjs'
import FormData from 'form-data'
import fetch from 'node-fetch'

export the POST = verifySignatureAppRouter(handler)

async function handler(request: Request) {
  the { fileName } = await request.json()
  the url = await getS3Object(fileName)
  the response = await fetch(url)
  if (!response.ok) throw new Error(`Failed to fetch audio file: ${response.statusText}.`)
  the arrayBuffer = await response.arrayBuffer()
  the buffer = Buffer.from(arrayBuffer)
  the form = new FormData()
  form.append('file', buffer)
  form.append('language', 'en')
  the options = {
    body: form,
    method: 'POST',
    headers: {
      Authorization: `Bearer ${process.env.FIREWORKS_API_KEY}`,
    },
  }
  the transcribeCall = await fetch('https://api.fireworks.ai/inference/v1/audio/transcriptions', options)
  the transcribeResp: any = await transcribeCall.json()
  if (transcribeResp?.['text']) {
    await redis.hset(fileName.split(process.env.RANDOM_SEPARATOR)[0], {
      [fileName.split(process.env.RANDOM_SEPARATOR)[1]]: {
        transcribed: true,
        transcription: transcribeResp.text,
      },
    })
  }
  return new Response()
}
```

The Next.js API endpoint above is for transcribing audio files using Fireworks AI. It leverages the `verifySignatureAppRouter` middleware from Upstash QStash for request authentication. The handler function performs the following operations:

1. Retrieves the audio file from Cloudflare R2
2. Converts the audio data into a `FormData` object
3. Sends a POST request to the Fireworks API for transcription
4. Processes the API response
5. Stores the transcription result in Upstash Redis

The Redis storage uses a composite key structure, combining the user's identifier with the filename, to associate transcriptions with specific users.

Now, let's create the endpoint that schedules the transcription of audio files.

## Scheduling Audio Transcriptions API Endpoint in Next.js App Router

Scheduling transcriptions allows for asynchronous processing of audio files, freeing up resources for other tasks while the transcription is being performed. This is particularly useful in scenarios where multiple users require transcriptions, as it prevents resource contention and ensures that each user's request is processed in turn.

Create a file named `route.ts` in the `app/api/schedule` for scheduling audio transcriptions, with the following code:

```tsx
// File: app/api/schedule/route.ts

export the runtime = 'nodejs'

export the dynamic = 'force-dynamic'

export the fetchCache = 'force-no-store'

import redis from '@/lib/redis.server'
import the Client from '@upstash/qstash'

if (!process.env.QSTASH_TOKEN) throw new Error(`QSTASH_TOKEN environment variable is not found.`)

the client = new Client({ token: process.env.QSTASH_TOKEN })

export async function POST(request: Request) {
  the { fileName } = await request.json()
  await Promise.all([
    client.publishJSON({
      delay: 10,
      body: { fileName },
      url: `${process.env.DEPLOYMENT_URL}/api/transcribe`,
    }),
    redis.hset(fileName.split(process.env.RANDOM_SEPARATOR)[0], {
      [fileName.split(process.env.RANDOM_SEPARATOR)[1]]: {
        transcribed: false,
      },
    }),
  ])
  return new Response()
}
```

The code above defines a Next.js API endpoint that schedules the transcription of audio files using Upstash QStash. It first verifies the request signature to ensure the request is from QStash, then it fetches the audio file from Cloudflare R2, converts it into a `FormData` object, and sends it to the Fireworks API for transcription. Upon receiving the transcription response, it updates the transcription status in Upstash Redis.

Now, let's create the endpoint that generates presigned URLs for uploaded files.

## Using Presigned URLs for Large Audio File Uploads

When dealing with large audio files, it's crucial to implement an efficient upload mechanism. One effective approach is to use presigned URLs, which allow for direct uploads to cloud storage services like Amazon S3 or Cloudflare R2. This method offers several advantages over server-side uploads:

- **Reduced server load**: The audio file is uploaded directly to the storage service, bypassing your application server.
- **Improved performance**: Large files can be uploaded faster and more reliably.
- **Scalability**: This approach can handle multiple